{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37481af2",
   "metadata": {},
   "source": [
    "sentiment  \n",
    "https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d21ea904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os \n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c7164f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/bettyliao/sentiment/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "651b7d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased') # 不區分大小寫\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "421b7598",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz' \n",
    "\n",
    "# tf.kera.utils.get_file 從網路下載資源\n",
    "dataset = tf.keras.utils.get_file(fname = 'aclImdb_v1.tar.gz',\n",
    "                                  origin = URL,\n",
    "                                  untar = True,\n",
    "                                  cache_dir = '.',\n",
    "                                  cache_subdir = ''\n",
    "                                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cd38f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unsupBow.feat', 'urls_pos.txt', 'urls_neg.txt', 'urls_unsup.txt', 'neg', 'pos', 'labeledBow.feat']\n"
     ]
    }
   ],
   "source": [
    "# create main directory path (/acIImdb)\n",
    "main_dir = os.path.join(os.path.dirname(dataset), 'aclImdb') \n",
    "# create sub directory path (/ac1Imdb/train)\n",
    "train_dir = os.path.join(main_dir, 'train')\n",
    "train_dir = '/home/bettyliao/sentiment/data' + train_dir.replace('.', '')\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)\n",
    "print(os.listdir(train_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00bd13ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train = tf.keras.preprocessing.text_dataset_from_directory('aclImdb/train', \n",
    "                                                           batch_size = 30000,\n",
    "                                                           validation_split = 0.2, # 驗證資料\n",
    "                                                           subset = 'training',\n",
    "                                                           seed = 123\n",
    "                                                          )\n",
    "test = tf.keras.preprocessing.text_dataset_from_directory('aclImdb/train',\n",
    "                                                          batch_size = 30000,\n",
    "                                                          validation_split = 0.2,\n",
    "                                                          subset = 'validation',\n",
    "                                                          seed = 123\n",
    "                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39952907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_feat:  20000 train_lab:  20000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_column</th>\n",
       "      <th>label_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canadian director Vincenzo Natali took the art...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I gave this film 10 not because it is a superb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I admit to being somewhat jaded about the movi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>For a long time, 'The Menagerie' was my favori...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A truly frightening film. Feels as if it were ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         data_column label_column\n",
       "0  Canadian director Vincenzo Natali took the art...            1\n",
       "1  I gave this film 10 not because it is a superb...            1\n",
       "2  I admit to being somewhat jaded about the movi...            1\n",
       "3  For a long time, 'The Menagerie' was my favori...            1\n",
       "4  A truly frightening film. Feels as if it were ...            0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in train.take(1):\n",
    "    train_feat = i[0].numpy()\n",
    "    train_lab = i[1].numpy()\n",
    "    print('train_feat: ', len(train_feat),\n",
    "         'train_lab: ', len(train_lab))\n",
    "    \n",
    "train = pd.DataFrame([train_feat, train_lab]).T # Transpose \n",
    "train.columns = ['data_column', 'label_column']\n",
    "train['data_column'] = train['data_column'].str.decode('utf-8')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d04be86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_column</th>\n",
       "      <th>label_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I can't believe that so much talent can be was...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This movie blows - let's get that straight rig...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The saddest thing about this \"tribute\" is that...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm only rating this film as a 3 out of pity b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Something surprised me about this movie - it w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         data_column label_column\n",
       "0  I can't believe that so much talent can be was...            0\n",
       "1  This movie blows - let's get that straight rig...            0\n",
       "2  The saddest thing about this \"tribute\" is that...            0\n",
       "3  I'm only rating this film as a 3 out of pity b...            0\n",
       "4  Something surprised me about this movie - it w...            1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for j in test.take(1):\n",
    "    test_feat = j[0].numpy()\n",
    "    test_lab = j[1].numpy()\n",
    "\n",
    "test = pd.DataFrame([test_feat, test_lab]).T\n",
    "test.columns = ['data_column', 'label_column']\n",
    "test['data_column'] = test['data_column'].str.decode('utf-8') \n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f90ade48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InputExample(guid=None, text_a='Hello world', text_b=None, label=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InputExample(guid = None,\n",
    "            text_a = 'Hello world',\n",
    "            text_b = None,\n",
    "            label = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43d7c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_examples(train, test, data_column, label_column):\n",
    "    train_InputExamples = train.apply(lambda x: InputExample(guid = None, \n",
    "                                                            text_a = x[data_column], \n",
    "                                                            text_b = None,\n",
    "                                                            label = x[label_column]),\n",
    "                                    axis = 1) \n",
    "    validation_InputExamples = test.apply(lambda x: InputExample(guid = None, \n",
    "                                                                text_a = x[data_column], \n",
    "                                                                text_b = None,\n",
    "                                                                label = x[label_column]),\n",
    "                                        axis = 1)\n",
    "    return train_InputExamples, validation_InputExamples\n",
    "\n",
    "train_input, test_input = convert_data_to_examples(train, test, 'data_column', 'label_column') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "614b7683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length = 128):\n",
    "    features = [] # will input inputfeatures to be convert later\n",
    "    \n",
    "    for e in examples:\n",
    "        input_dict = tokenizer.encode_plus(e.text_a,\n",
    "                                          add_special_tokens = True, \n",
    "                                          max_length = max_length, # truncates if len(s) > max_length \n",
    "                                          return_token_type_ids = True, \n",
    "                                          return_attention_mask = True,\n",
    "                                          pad_to_max_length = True, # pads to the right by default \n",
    "                                          truncation = True\n",
    "                                         )  \n",
    "        input_ids, token_type_ids, attention_mask = (input_dict['input_ids'], \n",
    "                                                    input_dict['token_type_ids'], \n",
    "                                                    input_dict['attention_mask']) \n",
    "        features.append(InputFeatures(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids,\n",
    "            label = e.label\n",
    "        ))\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield(\n",
    "                {\n",
    "                'input_ids': f.input_ids,\n",
    "                'attention_mask': f.attention_mask,\n",
    "                'token_type_ids': f.token_type_ids\n",
    "                },\n",
    "            f.label\n",
    "            )\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen, \n",
    "        ({\"input_ids\": tf.int32,\n",
    "          'attention_mask': tf.int32,\n",
    "          'token_type_ids': tf.int32\n",
    "            }, \n",
    "         tf.int64\n",
    "        ),\n",
    "        ({'input_ids': tf.TensorShape([None]),\n",
    "           'attention_mask': tf.TensorShape([None]),\n",
    "           'token_type_ids': tf.TensorShape([None])\n",
    "          },\n",
    "          tf.TensorShape([])\n",
    "         )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f9a5017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bettyliao/sentiment/sentiment/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "train_data = convert_examples_to_tf_dataset(list(train_input), tokenizer) \n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "\n",
    "validation_data = convert_examples_to_tf_dataset(list(test_input), tokenizer) \n",
    "validation_data = validation_data.batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c94a4d",
   "metadata": {},
   "source": [
    "## Configuring the BERT model and Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c59cf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1250/1250 [==============================] - 16646s 13s/step - loss: 0.2608 - accuracy: 0.8892 - val_loss: 0.3653 - val_accuracy: 0.8626\n",
      "Epoch 2/2\n",
      "1250/1250 [==============================] - 12595s 10s/step - loss: 0.0726 - accuracy: 0.9747 - val_loss: 0.5492 - val_accuracy: 0.8700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3778101908>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 3e-5, epsilon = 1e-08, clipnorm = 1.0),  \n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), \n",
    "              metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "model.fit(train_data, epochs = 2, validation_data = validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae83ef5c",
   "metadata": {},
   "source": [
    "## making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d59ca316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was an awesome movie. I watch it twice my time watching this beautiful movie if I known it was this good :\n",
      " Positive\n",
      "One of the worst movies of all time. I cannot believe I wasted two hours of my life for this movie :\n",
      " Negative\n"
     ]
    }
   ],
   "source": [
    "pred_sentences = ['This was an awesome movie. I watch it twice my time watching this beautiful movie if I known it was this good',\n",
    "                 'One of the worst movies of all time. I cannot believe I wasted two hours of my life for this movie'] \n",
    "tf_batch = tokenizer(pred_sentences, max_length = 128, padding = True, truncation = True, return_tensors = 'tf') # tensor use tensorflow \n",
    "tf_outputs = model(tf_batch)\n",
    "tf_predictions = tf.nn.softmax(tf_outputs[0], axis = -1)\n",
    "labels = ['Negative', 'Positive']\n",
    "label = tf.argmax(tf_predictions, axis = 1)\n",
    "label = label.numpy()\n",
    "for i in range(len(pred_sentences)):\n",
    "    print(pred_sentences[i], \":\\n\", labels[label[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a49906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22deec51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d581d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
